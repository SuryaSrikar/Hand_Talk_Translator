# Hand_Talk_Translator
This project is to helps the deaf person to communicate with a person using  tactile signs. This project mainly has two primary tasks machine learning part which helps to translate the image gestures into corresponding word or sentence. Using tensorflow lite we deploy this machine learning model on to to the Android mobile device.   

# Output of Trained deep learning model
Here in the following example you can see the output prediction of a input gesture to the model and it predicted the output correctly.

![c](https://user-images.githubusercontent.com/38751660/65381611-e8434680-dcc2-11e9-910d-4b17197a1226.JPG)
